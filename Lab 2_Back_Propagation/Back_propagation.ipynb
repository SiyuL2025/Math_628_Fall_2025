{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ad11ae8a",
      "metadata": {
        "id": "ad11ae8a"
      },
      "source": [
        "# Backprop from scratch — Placeholder (Student TODO)\n",
        "\n",
        "Implement the `TODO` sections in order:\n",
        "1. Losses (MSE, CrossEntropy)\n",
        "2. Layer forward/backward\n",
        "3. step() with **L2 regularization on weights only**\n",
        "4. train() mini-batch SGD + validation logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "97f9b44d",
      "metadata": {
        "id": "97f9b44d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "EPS = 1e-12\n",
        "\n",
        "def accuracy_from_logits(probs, y_onehot):\n",
        "    pred = np.argmax(probs, axis=1)\n",
        "    true = np.argmax(y_onehot, axis=1)\n",
        "    return np.mean(pred == true)\n",
        "class Loss:\n",
        "    def loss(self, x, y):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "class MSE(Loss):\n",
        "    # Mean over ALL elements (batch * out_dim)\n",
        "    def loss(self, x, y):\n",
        "        return np.mean(np.power(x - y, 2))\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        # derivative of mean((x-y)^2) is 2*(x-y)/x.size\n",
        "        return 2 * (x - y) / x.size\n",
        "\n",
        "class CrossEntropy(Loss):\n",
        "    # Mean over samples, sum over classes: mean( -sum(y*log(p)) )\n",
        "    def loss(self, x, y):\n",
        "        x = np.clip(x, EPS, 1.0)\n",
        "        return -np.mean(np.sum(y * np.log(x), axis=1))\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        x = np.clip(x, EPS, 1.0)\n",
        "        n = y.shape[0]\n",
        "        return -(y / x) / n\n",
        "\n",
        "class Layer:\n",
        "    def __init__(self, input_dim, output_dim, non_linearity=None) -> None:\n",
        "        self.in_dim = input_dim\n",
        "        self.out_dim = output_dim\n",
        "        self.non_linearity = non_linearity\n",
        "\n",
        "        self.output = None\n",
        "        self.input = None\n",
        "        self.grad_weight = None\n",
        "        self.grad_bias = None\n",
        "\n",
        "        # He init is fine for ReLU-ish nets\n",
        "        self.weights = np.random.randn(self.in_dim, self.out_dim) * np.sqrt(2 / self.in_dim)\n",
        "        self.bias = np.zeros(self.out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.input = x\n",
        "        z = x @ self.weights + self.bias\n",
        "\n",
        "        if self.non_linearity is None:\n",
        "            self.output = z\n",
        "        elif self.non_linearity == \"relu\":\n",
        "            self.output = np.maximum(0, z)\n",
        "        elif self.non_linearity == \"tanh\":\n",
        "            self.output = np.tanh(z)\n",
        "        elif self.non_linearity == \"soft_max\":\n",
        "            # stable softmax\n",
        "            z = z - np.max(z, axis=1, keepdims=True)\n",
        "            exp_z = np.exp(z)\n",
        "            self.output = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown non_linearity: {self.non_linearity}\")\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, gradients):\n",
        "        # gradients is dL/d(output_of_this_layer)\n",
        "        if self.non_linearity is None:\n",
        "            grad = gradients\n",
        "        elif self.non_linearity == \"relu\":\n",
        "            grad = (self.output > 0) * gradients\n",
        "        elif self.non_linearity == \"tanh\":\n",
        "            grad = (1 - self.output ** 2) * gradients\n",
        "        elif self.non_linearity == \"soft_max\":\n",
        "            # Jacobian-vector product for softmax\n",
        "            # dL/dz = s * (g - sum(g*s))\n",
        "            s = self.output\n",
        "            grad = s * (gradients - np.sum(gradients * s, axis=1, keepdims=True))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown non_linearity: {self.non_linearity}\")\n",
        "\n",
        "        # parameter gradients\n",
        "        self.grad_weight = self.input.T @ grad\n",
        "        self.grad_bias = np.sum(grad, axis=0)\n",
        "\n",
        "        # propagate to previous layer\n",
        "        grad_to_input = grad @ self.weights.T\n",
        "        return grad_to_input\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, in_dim, layers, out_dim, loss) -> None:\n",
        "        self.layers_size = layers\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.loss = loss\n",
        "        self.layers = []\n",
        "        prev = self.in_dim\n",
        "        for h in self.layers_size:\n",
        "            self.layers.append(Layer(prev, h, non_linearity='relu'))\n",
        "            prev = h\n",
        "\n",
        "        if loss == \"mse\":\n",
        "            self.layers.append(Layer(prev, self.out_dim, non_linearity=None))\n",
        "            self.loss = MSE()\n",
        "        elif loss == \"cross_entropy\":\n",
        "            self.layers.append(Layer(prev, self.out_dim, non_linearity=\"soft_max\"))\n",
        "            self.loss = CrossEntropy()\n",
        "        else:\n",
        "            raise ValueError(\"loss must be 'mse' or 'cross_entropy'\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, gradients):\n",
        "        for layer in reversed(self.layers):\n",
        "            gradients = layer.backward(gradients)\n",
        "\n",
        "    def step(self, lr):\n",
        "        for layer in self.layers:\n",
        "            layer.weights -= lr * layer.grad_weight\n",
        "            layer.bias -= lr * layer.grad_bias\n",
        "\n",
        "    def train(self, x, y, epochs, lr, batch_size=64, verbose_every=1 , x_val = None , y_val = None):\n",
        "        n = x.shape[0]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0.0  # 初始化损失总和为0\n",
        "            n_batches = 0\n",
        "\n",
        "            #suffhle\n",
        "            idx = np.random.permutation(n)\n",
        "            x_shuf = x[idx]  # 定义 x_shuf\n",
        "            y_shuf = y[idx]\n",
        "\n",
        "            for start in range(0, n, batch_size):\n",
        "                end = min(start + batch_size, n)\n",
        "                xb = x_shuf[start:end]\n",
        "                yb = y_shuf[start:end]\n",
        "\n",
        "               # implement training section\n",
        "                y_pred = self.forward(xb)\n",
        "                loss = self.loss.loss(y_pred,yb)\n",
        "                grad_to_loss = self.loss.gradient(y_pred, yb)\n",
        "                self.backward(grad_to_loss)\n",
        "                self.step(lr)\n",
        "                total_loss += loss\n",
        "                n_batches += 1\n",
        "\n",
        "            if (epoch + 1) % verbose_every == 0:\n",
        "\n",
        "                out_string = f\"Epoch {epoch+1}/{epochs} - Train_Loss: {total_loss / n_batches:.4f} \"\n",
        "\n",
        "                if x_val is not None  :\n",
        "                  y_pred = self.forward(x_val)\n",
        "                  loss = self.loss.loss(y_pred, y_val)\n",
        "                  out_string += f\"Validation_Loss: {loss:.4f} \"\n",
        "                  if isinstance(self.loss , CrossEntropy) :\n",
        "                    train_accurcy = accuracy_from_logits(y_pred ,y_val )\n",
        "                    out_string += f\"Validation_acc: {train_accurcy:.4f}\"\n",
        "\n",
        "                print(out_string)\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.forward(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1437d286",
      "metadata": {
        "id": "1437d286",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83a5efd1-959f-4b6f-c70b-218f6e8749fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred shape: (5, 3)\n",
            "Layer 0: W (4, 8), dW (4, 8), b (8,), db (8,)\n",
            "Layer 1: W (8, 8), dW (8, 8), b (8,), db (8,)\n",
            "Layer 2: W (8, 3), dW (8, 3), b (3,), db (3,)\n",
            "loss: 4.012321094108816\n"
          ]
        }
      ],
      "source": [
        "# Sanity check\n",
        "X = np.random.randn(5, 4)              # batch=5, in_dim=4\n",
        "y = np.random.randn(5, 3)              # regression target for mse (batch=5, out_dim=3)\n",
        "\n",
        "net = NeuralNetwork(in_dim=4, layers=[8, 8], out_dim=3, loss=\"mse\")\n",
        "pred = net.forward(X)\n",
        "print(\"pred shape:\", pred.shape)\n",
        "\n",
        "loss_val = net.loss.loss(pred, y)\n",
        "grads = net.loss.gradient(pred, y)\n",
        "net.backward(grads)\n",
        "\n",
        "for i, layer in enumerate(net.layers):\n",
        "    print(f\"Layer {i}: W {layer.weights.shape}, dW {layer.grad_weight.shape}, b {layer.bias.shape}, db {layer.grad_bias.shape}\")\n",
        "print(\"loss:\", loss_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "-fq3SjMefS9I",
      "metadata": {
        "id": "-fq3SjMefS9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a4ea5a-92be-45bc-cf2f-1af0bfd1a63b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.944538403883561\n",
            "error: 2.207100314718342e-11\n"
          ]
        }
      ],
      "source": [
        "def one_hot(y, num_classes):\n",
        "    y = np.asarray(y).astype(int)\n",
        "    oh = np.zeros((y.shape[0], num_classes))\n",
        "    oh[np.arange(y.shape[0]), y] = 1.0\n",
        "    return oh\n",
        "\n",
        "\n",
        "\n",
        "def numerical_grad_weight(net, X, Y, layer_index=0, eps=1e-5):\n",
        "    # compute numerical gradient for W in net.layers[layer_index]\n",
        "    layer = net.layers[layer_index]\n",
        "    W = layer.weights\n",
        "    numgrad = np.zeros_like(W)\n",
        "\n",
        "    # baseline\n",
        "    base_pred = net.forward(X)\n",
        "    base_loss = net.loss.loss(base_pred, Y)\n",
        "\n",
        "    for i in range(W.shape[0]):\n",
        "        for j in range(W.shape[1]):\n",
        "            old = W[i, j]\n",
        "\n",
        "            W[i, j] = old + eps\n",
        "            lp = net.loss.loss(net.forward(X), Y)\n",
        "\n",
        "            W[i, j] = old - eps\n",
        "            lm = net.loss.loss(net.forward(X), Y)\n",
        "\n",
        "            numgrad[i, j] = (lp - lm) / (2 * eps)\n",
        "            W[i, j] = old\n",
        "\n",
        "    return numgrad, base_loss\n",
        "\n",
        "# small classification net for the check\n",
        "Xc = np.random.randn(6, 3)\n",
        "yc = one_hot(np.random.randint(0, 2, size=6), 2)\n",
        "\n",
        "netc = NeuralNetwork(in_dim=3, layers=[5], out_dim=2, loss=\"cross_entropy\")\n",
        "\n",
        "# analytic grad\n",
        "pred = netc.forward(Xc)\n",
        "loss_val = netc.loss.loss(pred, yc)\n",
        "grads = netc.loss.gradient(pred, yc)\n",
        "netc.backward(grads)\n",
        "analytic = netc.layers[0].grad_weight.copy()\n",
        "\n",
        "# numerical grad\n",
        "num, _ = numerical_grad_weight(netc, Xc, yc, layer_index=0, eps=1e-5)\n",
        "\n",
        "err = np.linalg.norm(analytic - num)\n",
        "print(\"Loss:\", loss_val)\n",
        "print(\"error:\", err)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "CyhuYTYafX63",
      "metadata": {
        "id": "CyhuYTYafX63",
        "outputId": "5c1eee66-787b-455e-d45d-46cedf993572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000 - Train_Loss: 69.1960 Validation_Loss: 43.6415 \n",
            "Epoch 2/1000 - Train_Loss: 43.5916 Validation_Loss: 39.8472 \n",
            "Epoch 3/1000 - Train_Loss: 41.2296 Validation_Loss: 41.9001 \n",
            "Epoch 4/1000 - Train_Loss: 38.0320 Validation_Loss: 36.0098 \n",
            "Epoch 5/1000 - Train_Loss: 37.4707 Validation_Loss: 37.4266 \n",
            "Epoch 6/1000 - Train_Loss: 36.6572 Validation_Loss: 56.6995 \n",
            "Epoch 7/1000 - Train_Loss: 36.8666 Validation_Loss: 36.3482 \n",
            "Epoch 8/1000 - Train_Loss: 34.9098 Validation_Loss: 34.0066 \n",
            "Epoch 9/1000 - Train_Loss: 33.8042 Validation_Loss: 34.3986 \n",
            "Epoch 10/1000 - Train_Loss: 34.2748 Validation_Loss: 33.2080 \n",
            "Epoch 11/1000 - Train_Loss: 33.2051 Validation_Loss: 34.3059 \n",
            "Epoch 12/1000 - Train_Loss: 32.7318 Validation_Loss: 42.4450 \n",
            "Epoch 13/1000 - Train_Loss: 32.4227 Validation_Loss: 37.5986 \n",
            "Epoch 14/1000 - Train_Loss: 32.4107 Validation_Loss: 35.7714 \n",
            "Epoch 15/1000 - Train_Loss: 32.2930 Validation_Loss: 32.6350 \n",
            "Epoch 16/1000 - Train_Loss: 31.4780 Validation_Loss: 31.7009 \n",
            "Epoch 17/1000 - Train_Loss: 31.0795 Validation_Loss: 31.1492 \n",
            "Epoch 18/1000 - Train_Loss: 31.0537 Validation_Loss: 30.3274 \n",
            "Epoch 19/1000 - Train_Loss: 30.9001 Validation_Loss: 38.4182 \n",
            "Epoch 20/1000 - Train_Loss: 30.8094 Validation_Loss: 35.4141 \n",
            "Epoch 21/1000 - Train_Loss: 30.6257 Validation_Loss: 32.6917 \n",
            "Epoch 22/1000 - Train_Loss: 30.2258 Validation_Loss: 35.1725 \n",
            "Epoch 23/1000 - Train_Loss: 29.7278 Validation_Loss: 34.2148 \n",
            "Epoch 24/1000 - Train_Loss: 30.1262 Validation_Loss: 33.2134 \n",
            "Epoch 25/1000 - Train_Loss: 29.6666 Validation_Loss: 40.5882 \n",
            "Epoch 26/1000 - Train_Loss: 29.5025 Validation_Loss: 56.0491 \n",
            "Epoch 27/1000 - Train_Loss: 29.0408 Validation_Loss: 32.0495 \n",
            "Epoch 28/1000 - Train_Loss: 29.1286 Validation_Loss: 29.4616 \n",
            "Epoch 29/1000 - Train_Loss: 28.7539 Validation_Loss: 37.6446 \n",
            "Epoch 30/1000 - Train_Loss: 28.9510 Validation_Loss: 29.9796 \n",
            "Epoch 31/1000 - Train_Loss: 28.6171 Validation_Loss: 29.6487 \n",
            "Epoch 32/1000 - Train_Loss: 28.4748 Validation_Loss: 35.2811 \n",
            "Epoch 33/1000 - Train_Loss: 28.0973 Validation_Loss: 34.7011 \n",
            "Epoch 34/1000 - Train_Loss: 28.1118 Validation_Loss: 33.7040 \n",
            "Epoch 35/1000 - Train_Loss: 28.2070 Validation_Loss: 36.3993 \n",
            "Epoch 36/1000 - Train_Loss: 28.2763 Validation_Loss: 32.4488 \n",
            "Epoch 37/1000 - Train_Loss: 28.0627 Validation_Loss: 30.1334 \n",
            "Epoch 38/1000 - Train_Loss: 27.7379 Validation_Loss: 30.9240 \n",
            "Epoch 39/1000 - Train_Loss: 27.5267 Validation_Loss: 30.2137 \n",
            "Epoch 40/1000 - Train_Loss: 27.8228 Validation_Loss: 33.9893 \n",
            "Epoch 41/1000 - Train_Loss: 27.3978 Validation_Loss: 29.3022 \n",
            "Epoch 42/1000 - Train_Loss: 27.8021 Validation_Loss: 30.1200 \n",
            "Epoch 43/1000 - Train_Loss: 27.3879 Validation_Loss: 29.8594 \n",
            "Epoch 44/1000 - Train_Loss: 27.3553 Validation_Loss: 31.1605 \n",
            "Epoch 45/1000 - Train_Loss: 27.1916 Validation_Loss: 32.5414 \n",
            "Epoch 46/1000 - Train_Loss: 27.4198 Validation_Loss: 32.4862 \n",
            "Epoch 47/1000 - Train_Loss: 27.1998 Validation_Loss: 29.2812 \n",
            "Epoch 48/1000 - Train_Loss: 27.2488 Validation_Loss: 37.7523 \n",
            "Epoch 49/1000 - Train_Loss: 26.8576 Validation_Loss: 28.9511 \n",
            "Epoch 50/1000 - Train_Loss: 26.9523 Validation_Loss: 30.3099 \n",
            "Epoch 51/1000 - Train_Loss: 27.0322 Validation_Loss: 29.3404 \n",
            "Epoch 52/1000 - Train_Loss: 26.9308 Validation_Loss: 33.2136 \n",
            "Epoch 53/1000 - Train_Loss: 26.7751 Validation_Loss: 30.7990 \n",
            "Epoch 54/1000 - Train_Loss: 26.3568 Validation_Loss: 28.8261 \n",
            "Epoch 55/1000 - Train_Loss: 26.5500 Validation_Loss: 31.0615 \n",
            "Epoch 56/1000 - Train_Loss: 26.9990 Validation_Loss: 30.5876 \n",
            "Epoch 57/1000 - Train_Loss: 26.3935 Validation_Loss: 28.9278 \n",
            "Epoch 58/1000 - Train_Loss: 26.7726 Validation_Loss: 30.4209 \n",
            "Epoch 59/1000 - Train_Loss: 26.4363 Validation_Loss: 33.4298 \n",
            "Epoch 60/1000 - Train_Loss: 26.3895 Validation_Loss: 29.0363 \n",
            "Epoch 61/1000 - Train_Loss: 26.1978 Validation_Loss: 29.0567 \n",
            "Epoch 62/1000 - Train_Loss: 26.2737 Validation_Loss: 31.8337 \n",
            "Epoch 63/1000 - Train_Loss: 26.1542 Validation_Loss: 28.6139 \n",
            "Epoch 64/1000 - Train_Loss: 26.1975 Validation_Loss: 30.0985 \n",
            "Epoch 65/1000 - Train_Loss: 26.1662 Validation_Loss: 28.7649 \n",
            "Epoch 66/1000 - Train_Loss: 25.8733 Validation_Loss: 31.4575 \n",
            "Epoch 67/1000 - Train_Loss: 26.0565 Validation_Loss: 31.5576 \n",
            "Epoch 68/1000 - Train_Loss: 26.1631 Validation_Loss: 29.4260 \n",
            "Epoch 69/1000 - Train_Loss: 25.8925 Validation_Loss: 30.2456 \n",
            "Epoch 70/1000 - Train_Loss: 25.7721 Validation_Loss: 34.3068 \n",
            "Epoch 71/1000 - Train_Loss: 25.8779 Validation_Loss: 29.0447 \n",
            "Epoch 72/1000 - Train_Loss: 25.7091 Validation_Loss: 29.1622 \n",
            "Epoch 73/1000 - Train_Loss: 25.9643 Validation_Loss: 28.7470 \n",
            "Epoch 74/1000 - Train_Loss: 25.6811 Validation_Loss: 28.8670 \n",
            "Epoch 75/1000 - Train_Loss: 25.7250 Validation_Loss: 28.9233 \n",
            "Epoch 76/1000 - Train_Loss: 25.7522 Validation_Loss: 30.6355 \n",
            "Epoch 77/1000 - Train_Loss: 25.7558 Validation_Loss: 32.5255 \n",
            "Epoch 78/1000 - Train_Loss: 25.5075 Validation_Loss: 31.7410 \n",
            "Epoch 79/1000 - Train_Loss: 25.5932 Validation_Loss: 29.1790 \n",
            "Epoch 80/1000 - Train_Loss: 25.4381 Validation_Loss: 33.8355 \n",
            "Epoch 81/1000 - Train_Loss: 25.3243 Validation_Loss: 30.5390 \n",
            "Epoch 82/1000 - Train_Loss: 25.3829 Validation_Loss: 28.1730 \n",
            "Epoch 83/1000 - Train_Loss: 25.2473 Validation_Loss: 29.0297 \n",
            "Epoch 84/1000 - Train_Loss: 25.5096 Validation_Loss: 28.7375 \n",
            "Epoch 85/1000 - Train_Loss: 25.2569 Validation_Loss: 31.8780 \n",
            "Epoch 86/1000 - Train_Loss: 25.0111 Validation_Loss: 28.8090 \n",
            "Epoch 87/1000 - Train_Loss: 25.1826 Validation_Loss: 28.7517 \n",
            "Epoch 88/1000 - Train_Loss: 25.2672 Validation_Loss: 28.5574 \n",
            "Epoch 89/1000 - Train_Loss: 25.0120 Validation_Loss: 30.4191 \n",
            "Epoch 90/1000 - Train_Loss: 25.0999 Validation_Loss: 28.8113 \n",
            "Epoch 91/1000 - Train_Loss: 25.0555 Validation_Loss: 29.4265 \n",
            "Epoch 92/1000 - Train_Loss: 24.8689 Validation_Loss: 28.9067 \n",
            "Epoch 93/1000 - Train_Loss: 25.1070 Validation_Loss: 29.0533 \n",
            "Epoch 94/1000 - Train_Loss: 24.9558 Validation_Loss: 29.9713 \n",
            "Epoch 95/1000 - Train_Loss: 24.9375 Validation_Loss: 30.8379 \n",
            "Epoch 96/1000 - Train_Loss: 25.0705 Validation_Loss: 29.7646 \n",
            "Epoch 97/1000 - Train_Loss: 24.8225 Validation_Loss: 29.1421 \n",
            "Epoch 98/1000 - Train_Loss: 24.9683 Validation_Loss: 29.5100 \n",
            "Epoch 99/1000 - Train_Loss: 24.9164 Validation_Loss: 36.3235 \n",
            "Epoch 100/1000 - Train_Loss: 24.9069 Validation_Loss: 28.5424 \n",
            "Epoch 101/1000 - Train_Loss: 24.9800 Validation_Loss: 28.4291 \n",
            "Epoch 102/1000 - Train_Loss: 24.7526 Validation_Loss: 31.7487 \n",
            "Epoch 103/1000 - Train_Loss: 24.5570 Validation_Loss: 27.8051 \n",
            "Epoch 104/1000 - Train_Loss: 24.6872 Validation_Loss: 29.9168 \n",
            "Epoch 105/1000 - Train_Loss: 24.6584 Validation_Loss: 29.7667 \n",
            "Epoch 106/1000 - Train_Loss: 24.6057 Validation_Loss: 28.7249 \n",
            "Epoch 107/1000 - Train_Loss: 24.7820 Validation_Loss: 34.3397 \n",
            "Epoch 108/1000 - Train_Loss: 24.5255 Validation_Loss: 28.7575 \n",
            "Epoch 109/1000 - Train_Loss: 24.2895 Validation_Loss: 29.6911 \n",
            "Epoch 110/1000 - Train_Loss: 24.6281 Validation_Loss: 27.8811 \n",
            "Epoch 111/1000 - Train_Loss: 24.3894 Validation_Loss: 28.3434 \n",
            "Epoch 112/1000 - Train_Loss: 24.4887 Validation_Loss: 28.7830 \n",
            "Epoch 113/1000 - Train_Loss: 24.3699 Validation_Loss: 29.4038 \n",
            "Epoch 114/1000 - Train_Loss: 24.4844 Validation_Loss: 28.0447 \n",
            "Epoch 115/1000 - Train_Loss: 24.6298 Validation_Loss: 33.0477 \n",
            "Epoch 116/1000 - Train_Loss: 24.5167 Validation_Loss: 28.5328 \n",
            "Epoch 117/1000 - Train_Loss: 24.0985 Validation_Loss: 27.6198 \n",
            "Epoch 118/1000 - Train_Loss: 24.1928 Validation_Loss: 29.0532 \n",
            "Epoch 119/1000 - Train_Loss: 24.2641 Validation_Loss: 28.9374 \n",
            "Epoch 120/1000 - Train_Loss: 24.3109 Validation_Loss: 33.7113 \n",
            "Epoch 121/1000 - Train_Loss: 24.3834 Validation_Loss: 41.7478 \n",
            "Epoch 122/1000 - Train_Loss: 24.1520 Validation_Loss: 29.7337 \n",
            "Epoch 123/1000 - Train_Loss: 24.2678 Validation_Loss: 30.2964 \n",
            "Epoch 124/1000 - Train_Loss: 24.2043 Validation_Loss: 28.2920 \n",
            "Epoch 125/1000 - Train_Loss: 24.0474 Validation_Loss: 27.8267 \n",
            "Epoch 126/1000 - Train_Loss: 24.2049 Validation_Loss: 28.0494 \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2247295095.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m\"mse\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mverbose_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2221164962.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, epochs, lr, batch_size, verbose_every, x_val, y_val)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                \u001b[0;31m# implement training section\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mgrad_to_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2221164962.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2221164962.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_linearity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnon_linearity\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tanh\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n",
        "\n",
        "X , y = df.iloc[: , :-1].values , df.iloc[: , -1 : ].values /10000\n",
        "\n",
        "X = (X  - np.mean(X , axis= 0 )) / np.std(X , axis= 0 ) #这里使用了未来信息\n",
        "perm = np.random.permutation(X.shape[0])\n",
        "X_shuf, y_shuf = X[perm], y[perm]\n",
        "\n",
        "split = int(X.shape[0] * 0.8)\n",
        "X_train , y_train = X_shuf[:split] , y_shuf[:split]\n",
        "X_val , y_val = X_shuf[split:] , y_shuf[split:]\n",
        "\n",
        "model = NeuralNetwork(X.shape[1] , [16  , 32 , 32 , 16] , y.shape[1] , \"mse\")\n",
        "\n",
        "model.train(X_train , y_train , 1000 , 0.001 , 64 , verbose_every=1 , x_val = X_val , y_val = y_val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "jHiPdc0ofZdH",
      "metadata": {
        "id": "jHiPdc0ofZdH"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(\"/content/sample_data/mnist_train_small.csv\")\n",
        "\n",
        "X , y = df.iloc[: ,1 : ].values , df.iloc[: , 0 ].values\n",
        "\n",
        "n_class = len(set(y))\n",
        "y = one_hot(y , n_class)\n",
        "\n",
        "perm = np.random.permutation(X.shape[0])\n",
        "X_shuf, y_shuf = X[perm], y[perm]\n",
        "\n",
        "split = int(X.shape[0] * 0.8)\n",
        "X_train , y_train = X_shuf[:split] , y_shuf[:split]\n",
        "X_val , y_val = X_shuf[split:] , y_shuf[split:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B4677SxafbDv",
      "metadata": {
        "id": "B4677SxafbDv",
        "outputId": "7fdc3d9f-7a44-47d5-ca99-7a38b7c11774",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000 - Train_Loss: 5.9399 Validation_Loss: 1.8892 Validation_acc: 0.3513\n",
            "Epoch 2/1000 - Train_Loss: 1.7324 Validation_Loss: 1.5415 Validation_acc: 0.4743\n",
            "Epoch 3/1000 - Train_Loss: 1.5036 Validation_Loss: 1.4000 Validation_acc: 0.4938\n",
            "Epoch 4/1000 - Train_Loss: 1.3761 Validation_Loss: 1.2958 Validation_acc: 0.5635\n",
            "Epoch 5/1000 - Train_Loss: 1.2644 Validation_Loss: 1.2305 Validation_acc: 0.5715\n",
            "Epoch 6/1000 - Train_Loss: 1.1492 Validation_Loss: 1.0740 Validation_acc: 0.6465\n",
            "Epoch 7/1000 - Train_Loss: 1.0877 Validation_Loss: 1.4850 Validation_acc: 0.5813\n",
            "Epoch 8/1000 - Train_Loss: 1.0555 Validation_Loss: 1.1121 Validation_acc: 0.6345\n",
            "Epoch 9/1000 - Train_Loss: 1.0175 Validation_Loss: 0.9494 Validation_acc: 0.6745\n",
            "Epoch 10/1000 - Train_Loss: 0.9863 Validation_Loss: 0.9347 Validation_acc: 0.6835\n",
            "Epoch 11/1000 - Train_Loss: 0.9578 Validation_Loss: 0.9373 Validation_acc: 0.6975\n",
            "Epoch 12/1000 - Train_Loss: 0.9484 Validation_Loss: 0.9223 Validation_acc: 0.6937\n",
            "Epoch 13/1000 - Train_Loss: 0.9165 Validation_Loss: 0.8562 Validation_acc: 0.7123\n",
            "Epoch 14/1000 - Train_Loss: 0.8910 Validation_Loss: 0.8457 Validation_acc: 0.7165\n",
            "Epoch 15/1000 - Train_Loss: 0.8553 Validation_Loss: 0.8166 Validation_acc: 0.7195\n",
            "Epoch 16/1000 - Train_Loss: 0.8310 Validation_Loss: 0.7740 Validation_acc: 0.7542\n",
            "Epoch 17/1000 - Train_Loss: 0.8082 Validation_Loss: 0.7575 Validation_acc: 0.7470\n",
            "Epoch 18/1000 - Train_Loss: 0.7853 Validation_Loss: 0.7722 Validation_acc: 0.7508\n",
            "Epoch 19/1000 - Train_Loss: 0.7621 Validation_Loss: 0.7535 Validation_acc: 0.7595\n",
            "Epoch 20/1000 - Train_Loss: 0.7485 Validation_Loss: 0.7170 Validation_acc: 0.7675\n",
            "Epoch 21/1000 - Train_Loss: 0.7294 Validation_Loss: 0.7258 Validation_acc: 0.7655\n",
            "Epoch 22/1000 - Train_Loss: 0.7166 Validation_Loss: 0.6885 Validation_acc: 0.7808\n",
            "Epoch 23/1000 - Train_Loss: 0.7062 Validation_Loss: 1.0125 Validation_acc: 0.6820\n",
            "Epoch 24/1000 - Train_Loss: 0.6944 Validation_Loss: 0.6800 Validation_acc: 0.7810\n",
            "Epoch 25/1000 - Train_Loss: 0.6891 Validation_Loss: 0.6548 Validation_acc: 0.8037\n",
            "Epoch 26/1000 - Train_Loss: 0.6713 Validation_Loss: 0.7100 Validation_acc: 0.7670\n",
            "Epoch 27/1000 - Train_Loss: 0.6663 Validation_Loss: 0.6529 Validation_acc: 0.7965\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork(X.shape[1] , [16  , 32 , 32 , 16] , y.shape[1] , \"cross_entropy\")\n",
        "\n",
        "model.train(X , y , 1000 , 0.001, 64 , verbose_every=1 , x_val = X_val , y_val = y_val)\n",
        "\n",
        "#这个训练就是正常的"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}